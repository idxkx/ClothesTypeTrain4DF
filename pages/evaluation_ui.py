# Placeholder for evaluation UI code 

import streamlit as st
import torch
import os
import json
import pandas as pd
from PIL import Image
import traceback
import math
from torchvision import transforms

# --- å¸¸é‡ ---
RESULTS_FILE = "../training_results.json" # ç»“æžœæ–‡ä»¶ç›¸å¯¹äºŽ pages ç›®å½•
CATEGORY_FILE = "list_category_cloth.txt"
ATTRIBUTE_FILE = "list_attr_cloth.txt"
# --- æ–°å¢žï¼šæ˜ å°„æ–‡ä»¶å¸¸é‡ --- 
MAPPING_FILE = "../name_mapping.json" 

# æ·»åŠ é»˜è®¤Anno_fineè·¯å¾„å¸¸é‡
DEFAULT_ANNO_DIR = r"E:\AIModels\DeepFashion\DeepFashion\Category and Attribute Prediction Benchmark\Anno_fine"

# --- æ¨¡åž‹å¯¼å…¥ ---
# å‡è®¾ model.py åœ¨é¡¹ç›®æ ¹ç›®å½•
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
try:
    from model import ClothesModel
except ImportError as e:
    st.error(f"é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ ClothesModelã€‚è¯·ç¡®ä¿ model.py åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚é”™è¯¯: {e}")
    st.stop()

# --- è¾…åŠ©å‡½æ•° ---
# (ä»Ž main_ui.py å¤åˆ¶å¹¶å¯èƒ½ç¨ä½œä¿®æ”¹)
def load_results():
    """åŠ è½½åŽ†å²è®­ç»ƒç»“æžœ"""
    results_path = os.path.join(os.path.dirname(__file__), RESULTS_FILE)
    try:
        with open(results_path, 'r', encoding='utf-8') as f:
            results = json.load(f)
        return results if isinstance(results, list) else []
    except FileNotFoundError:
        st.warning(f"æœªæ‰¾åˆ°è®­ç»ƒç»“æžœæ–‡ä»¶: {results_path}ã€‚è¯·å…ˆåœ¨ä¸»é¡µé¢å®Œæˆè‡³å°‘ä¸€æ¬¡è®­ç»ƒã€‚")
        return []
    except json.JSONDecodeError:
        st.error(f"è®­ç»ƒç»“æžœæ–‡ä»¶ {results_path} æ ¼å¼é”™è¯¯ã€‚")
        return []

def load_category_names(anno_dir):
    """ä»Ž Anno_fine ç›®å½•åŠ è½½ç±»åˆ«åç§°"""
    file_path = os.path.join(anno_dir, CATEGORY_FILE)
    names = {}
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()
            for i, line in enumerate(lines[2:]):
                parts = line.strip().split()
                if len(parts) >= 1:
                    category_name = ' '.join(parts[:-1])
                    names[i + 1] = category_name # Key is ID (1-50)
    except FileNotFoundError:
        st.error(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç±»åˆ«æ–‡ä»¶ {file_path}")
        return None
    except Exception as e:
        st.error(f"è¯»å–ç±»åˆ«æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return None
    if len(names) != 50:
        st.warning(f"è­¦å‘Šï¼šä»Ž {file_path} åŠ è½½äº† {len(names)} ä¸ªç±»åˆ«åç§°ï¼Œé¢„æœŸæ˜¯ 50 ä¸ªã€‚")
    return names

def load_attribute_names(anno_dir):
    """ä»Ž Anno_fine ç›®å½•åŠ è½½å±žæ€§åç§°"""
    file_path = os.path.join(anno_dir, ATTRIBUTE_FILE)
    names = {}
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()
            for i, line in enumerate(lines[2:]):
                parts = line.strip().split()
                if len(parts) >= 1:
                    attribute_name = ' '.join(parts[:-1])
                    names[i] = attribute_name # Key is index (0-999)
    except FileNotFoundError:
        st.error(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°å±žæ€§æ–‡ä»¶ {file_path}")
        return None
    except Exception as e:
        st.error(f"è¯»å–å±žæ€§æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return None
    # --- ä¿®æ”¹ï¼šæ›´æ–°é¢„æœŸçš„å±žæ€§æ•°é‡ --- 
    expected_attrs = 26
    if len(names) != expected_attrs:
         st.warning(f"è­¦å‘Šï¼šä»Ž {file_path} åŠ è½½äº† {len(names)} ä¸ªå±žæ€§åç§°ï¼Œé¢„æœŸæ˜¯ {expected_attrs} ä¸ªã€‚")
    return names

# --- æ–°å¢žï¼šåŠ è½½åç§°æ˜ å°„ --- 
def load_name_mapping():
    mapping_path = os.path.join(os.path.dirname(__file__), MAPPING_FILE)
    try:
        with open(mapping_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        # æä¾›é»˜è®¤ç©ºå­—å…¸ï¼Œé˜²æ­¢æ–‡ä»¶ä¸åŒ…å«æŸä¸ªé”®
        return mapping.get("categories", {}), mapping.get("attributes", {})
    except FileNotFoundError:
        st.info(f"æç¤ºï¼šæœªæ‰¾åˆ°åç§°æ˜ å°„æ–‡ä»¶ {mapping_path}ï¼Œå°†ä»…æ˜¾ç¤ºè‹±æ–‡åç§°ã€‚")
        return {}, {}
    except json.JSONDecodeError:
        st.error(f"åç§°æ˜ å°„æ–‡ä»¶ {mapping_path} æ ¼å¼é”™è¯¯ã€‚")
        return {}, {}

# --- åŠ è½½æ˜ å°„ --- 
category_mapping, attribute_mapping = load_name_mapping()

# å›¾åƒé¢„å¤„ç† (ä¸ŽéªŒè¯é›†ç›¸åŒ)
# TODO: ä½¿å›¾åƒå¤§å°å’Œå½’ä¸€åŒ–å‚æ•°ä¸Žè®­ç»ƒé…ç½®ä¸€è‡´ (å¯ä»¥ä»Ž results.json èŽ·å–?)
img_size = 224
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
eval_transform = transforms.Compose([
    transforms.Resize((img_size, img_size)),
    transforms.ToTensor(),
    normalize,
])

# --- UI å¸ƒå±€ ---
st.set_page_config(page_title="æ¨¡åž‹æ•ˆæžœæµ‹è¯•", layout="wide")
st.title("ðŸ§ª æ¨¡åž‹æ•ˆæžœæµ‹è¯•")
st.markdown("ä»Žè®­ç»ƒå¥½çš„æ¨¡åž‹ä¸­é€‰æ‹©ä¸€ä¸ªï¼Œä¸Šä¼ æœè£…å›¾ç‰‡ï¼ŒæŸ¥çœ‹è¯†åˆ«ç»“æžœã€‚")

# è®¾ç½®åˆå§‹çŠ¶æ€ï¼Œé»˜è®¤ä¸ºä¸‹æ‹‰åˆ—è¡¨é€‰æ‹©æ¨¡å¼
if "using_dropdown_selection" not in st.session_state:
    st.session_state.using_dropdown_selection = True

# --- åŠ è½½å’Œé€‰æ‹©æ¨¡åž‹ ---
all_results = load_results()
# ç­›é€‰å‡ºæˆåŠŸçš„è®­ç»ƒè¿è¡Œ
successful_runs = [
    r for r in all_results
    if r.get("status") == "å·²å®Œæˆ" and
       r.get("functional_test_result") == "æˆåŠŸ" and
       r.get("best_model_path") and
       os.path.exists(os.path.join(os.path.dirname(__file__), '..', r["best_model_path"])) # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
]

model_options = {"è¯·é€‰æ‹©æ¨¡åž‹": None}
if successful_runs:
    for run in sorted(successful_runs, key=lambda x: x.get("end_time", 0), reverse=True):
        option_label = f"{run.get('model_name', 'æœªçŸ¥æ¨¡åž‹')} (å®ŒæˆäºŽ {run.get('end_time_str', 'æœªçŸ¥æ—¶é—´')}, Backbone: {run.get('backbone', 'æœªçŸ¥')})"
        model_options[option_label] = {
            "path": os.path.join(os.path.dirname(__file__), '..', run["best_model_path"]), 
            "backbone": run.get("backbone"),
            # ä½¿ç”¨é»˜è®¤è·¯å¾„ä½œä¸ºå¤‡é€‰
            "anno_dir": run.get("anno_dir", DEFAULT_ANNO_DIR) 
        }
else:
    st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®­ç»ƒå¥½çš„æ¨¡åž‹è®°å½•ã€‚è¯·å…ˆåœ¨ä¸»é¡µé¢å®Œæˆæ¨¡åž‹è®­ç»ƒã€‚")

# ç®€åŒ–UIï¼Œåªä½¿ç”¨ä¸‹æ‹‰åˆ—è¡¨é€‰æ‹©æ¨¡åž‹
st.selectbox(
    "é€‰æ‹©ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡åž‹:",
    list(model_options.keys()),
    key="selected_model_dropdown"
)

selected_model_info = model_options.get(st.session_state.selected_model_dropdown)

# å¦‚æžœé€‰æ‹©äº†æ¨¡åž‹ï¼Œæ˜¾ç¤ºæ¨¡åž‹ä¿¡æ¯
if selected_model_info:
    st.success(f"å·²é€‰æ‹©æ¨¡åž‹ï¼Œéª¨å¹²ç½‘ç»œ: {selected_model_info.get('backbone', 'æœªçŸ¥')}")

# --- å›¾ç‰‡ä¸Šä¼  ---
uploaded_file = st.file_uploader(
    "ä¸Šä¼ ä¸€å¼ æœè£…å›¾ç‰‡:",
    type=["jpg", "jpeg", "png"]
)

# --- è¯†åˆ«æŒ‰é’®å’Œç»“æžœæ˜¾ç¤º ---
col_img, col_results = st.columns(2)

if uploaded_file is not None:
    try:
        image = Image.open(uploaded_file).convert('RGB')
        with col_img:
            st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", use_column_width=True)
    except Exception as e:
        st.error(f"æ— æ³•åŠ è½½å›¾ç‰‡: {e}")
        uploaded_file = None # é˜»æ­¢åŽç»­å¤„ç†

if st.button("ðŸš€ å¼€å§‹è¯†åˆ«ï¼"):
    # ç®€åŒ–é€»è¾‘ï¼Œåªå¤„ç†ä¸‹æ‹‰åˆ—è¡¨é€‰æ‹©çš„æƒ…å†µ
    if not selected_model_info:
        st.error("è¯·å…ˆä»Žä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡åž‹ã€‚")
    elif not uploaded_file:
        st.error("è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡ã€‚")
    else:
        # èŽ·å–æ¨¡åž‹ä¿¡æ¯
        path_to_use = selected_model_info.get("path")
        backbone_to_use = selected_model_info.get("backbone")
        final_anno_dir = selected_model_info.get("anno_dir")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        st.write(f"æ¨¡åž‹è·¯å¾„: {path_to_use}")
        st.write(f"éª¨å¹²ç½‘ç»œ: {backbone_to_use}")
        st.write(f"Annoç›®å½•: {final_anno_dir}")
        st.write(f"ç›®å½•å­˜åœ¨: {os.path.isdir(final_anno_dir) if final_anno_dir else False}")

        # ä½¿ç”¨å¸ƒå°”æ ‡å¿—æŽ§åˆ¶æµç¨‹
        should_proceed = True
        
        # æ£€æŸ¥å¿…è¦ä¿¡æ¯æ˜¯å¦å®Œæ•´
        if not path_to_use or not backbone_to_use:
            st.error("é”™è¯¯ï¼šé€‰æ‹©çš„æ¨¡åž‹ä¿¡æ¯ä¸å®Œæ•´ (è·¯å¾„æˆ–Backbone)ã€‚")
            should_proceed = False
        elif not final_anno_dir:
            st.error(f"é”™è¯¯ï¼šæ¨¡åž‹è®°å½•ä¸­æ²¡æœ‰Anno_fineç›®å½•è·¯å¾„ã€‚")
            should_proceed = False
        elif not os.path.isdir(final_anno_dir):
            # å¦‚æžœç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤ç›®å½•
            st.warning(f"æŒ‡å®šçš„Anno_fineç›®å½•ä¸å­˜åœ¨: {final_anno_dir}")
            if os.path.isdir(DEFAULT_ANNO_DIR):
                st.info(f"ä½¿ç”¨é»˜è®¤Anno_fineç›®å½•: {DEFAULT_ANNO_DIR}")
                final_anno_dir = DEFAULT_ANNO_DIR
            else:
                st.error(f"é»˜è®¤Anno_fineç›®å½•ä¹Ÿä¸å­˜åœ¨: {DEFAULT_ANNO_DIR}")
                should_proceed = False
        
        # åªæœ‰å½“æ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³æ—¶æ‰ç»§ç»­å¤„ç†
        if should_proceed:
            # æ‰€æœ‰ä¿¡æ¯éƒ½å®Œæ•´ï¼Œå¼€å§‹å¤„ç†
            model_path = path_to_use
            backbone = backbone_to_use
            num_categories = 50 
            num_attributes = 26 

            # åŠ è½½ç±»åˆ«å’Œå±žæ€§åç§°
            category_names_en = load_category_names(final_anno_dir)
            attribute_names_en = load_attribute_names(final_anno_dir)

            if category_names_en is None or attribute_names_en is None:
                st.error("æ— æ³•åŠ è½½ç±»åˆ«æˆ–å±žæ€§åç§°ï¼Œæ— æ³•ç»§ç»­è¯†åˆ«ã€‚")
            else:
                # å¼€å§‹è¯†åˆ«æµç¨‹
                with st.spinner("æ­£åœ¨åŠ è½½æ¨¡åž‹å¹¶è¿›è¡Œè¯†åˆ«..."):
                    try:
                        # 1. åŠ è½½æ¨¡åž‹
                        model = ClothesModel(num_categories=num_categories, backbone=backbone)
                        # å°è¯•è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
                        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                        model.load_state_dict(torch.load(model_path, map_location=device))
                        model.to(device)
                        model.eval()

                        # 2. é¢„å¤„ç†å›¾ç‰‡
                        img_tensor = eval_transform(image).unsqueeze(0).to(device) # æ·»åŠ  batch ç»´åº¦

                        # 3. æ¨¡åž‹æŽ¨ç†
                        with torch.no_grad():
                            cat_logits, attr_logits = model(img_tensor)

                        # 4. è§£æžç»“æžœ
                        # ç±»åˆ« - èŽ·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚çŽ‡åˆ†å¸ƒ
                        cat_probs = torch.softmax(cat_logits, dim=1).squeeze(0).cpu().numpy()
                        
                        # åˆ›å»ºç±»åˆ«ç´¢å¼•ã€åç§°å’Œæ¦‚çŽ‡çš„åˆ—è¡¨
                        cat_data = []
                        for idx in range(len(cat_probs)):
                            cat_id = idx + 1  # ç±»åˆ«IDä»Ž1å¼€å§‹
                            en_cat_name = category_names_en.get(cat_id, f"Unknown (ID: {cat_id})")
                            zh_cat_name = category_mapping.get(en_cat_name)
                            display_cat_name = f"{zh_cat_name} ({en_cat_name})" if zh_cat_name else en_cat_name
                            cat_data.append({
                                'index': cat_id,
                                'name': display_cat_name,
                                'probability': cat_probs[idx]
                            })
                        
                        # æŒ‰æ¦‚çŽ‡é™åºæŽ’åº
                        cat_data.sort(key=lambda x: x['probability'], reverse=True)
                        
                        # èŽ·å–æ¦‚çŽ‡æœ€é«˜çš„ç±»åˆ«
                        top_category = cat_data[0]
                        
                        # å±žæ€§ (ä½¿ç”¨ Sigmoid èŽ·å–æ‰€æœ‰å±žæ€§çš„æ¦‚çŽ‡)
                        attr_probs = torch.sigmoid(attr_logits).squeeze(0).cpu().numpy()
                        
                        # åˆ›å»ºå±žæ€§ç´¢å¼•ã€åç§°å’Œæ¦‚çŽ‡çš„åˆ—è¡¨
                        attr_data = []
                        for idx, prob in enumerate(attr_probs):
                            en_attr_name = attribute_names_en.get(idx, f"Unknown Attr (Idx: {idx})")
                            zh_attr_name = attribute_mapping.get(en_attr_name)
                            display_attr_name = f"{zh_attr_name} ({en_attr_name})" if zh_attr_name else en_attr_name
                            attr_data.append({
                                'index': idx,
                                'name': display_attr_name, 
                                'probability': prob
                            })
                        
                        # æŒ‰æ¦‚çŽ‡é™åºæŽ’åº
                        attr_data.sort(key=lambda x: x['probability'], reverse=True)
                        
                        # 5. æ˜¾ç¤ºç»“æžœ
                        with col_results:
                            st.subheader("è¯†åˆ«ç»“æžœ:")
                            
                            # æ˜¾ç¤ºç±»åˆ«é¢„æµ‹ç»“æžœ
                            st.markdown("**é¢„æµ‹ç±»åˆ«åŠæ¦‚çŽ‡:**")
                            # æ˜¾ç¤ºå‰3ä¸ªæœ€å¯èƒ½çš„ç±»åˆ«
                            cols_cat = st.columns(3)
                            for i, cat in enumerate(cat_data[:3]):
                                with cols_cat[i]:
                                    if i == 0:  # æœ€é«˜æ¦‚çŽ‡ç”¨ç»¿è‰²
                                        st.success(f"{cat['name']} ({cat['probability']*100:.1f}%)")
                                    else:  # å…¶ä»–å€™é€‰ç”¨è“è‰²
                                        st.info(f"{cat['name']} ({cat['probability']*100:.1f}%)")
                            
                            # ç±»åˆ«è¯¦æƒ…æŠ˜å é¢æ¿
                            with st.expander("æŸ¥çœ‹æ‰€æœ‰ç±»åˆ«æ¦‚çŽ‡è¯¦æƒ…"):
                                # æ˜¾ç¤ºå‰10ä¸ªæœ€å¯èƒ½çš„ç±»åˆ«
                                st.markdown("##### å‰10ä¸ªæœ€å¯èƒ½çš„ç±»åˆ«:")
                                cat_top10_df = pd.DataFrame(cat_data[:10])
                                cat_top10_df.columns = ["ID", "ç±»åˆ«åç§°", "æ¦‚çŽ‡"]
                                cat_top10_df["æ¦‚çŽ‡"] = cat_top10_df["æ¦‚çŽ‡"].apply(lambda x: f"{x*100:.2f}%")
                                st.dataframe(cat_top10_df)
                                
                                # æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚çŽ‡åˆ†å¸ƒå›¾
                                st.markdown("##### ç±»åˆ«æ¦‚çŽ‡åˆ†å¸ƒ:")
                                if len(cat_data) > 10:
                                    fig_data = pd.DataFrame({
                                        'ç±»åˆ«': [d['name'].split(' ')[0] for d in cat_data[:10]] + ['å…¶ä»–'],
                                        'æ¦‚çŽ‡': [d['probability'] for d in cat_data[:10]] + [sum(d['probability'] for d in cat_data[10:])]
                                    })
                                else:
                                    fig_data = pd.DataFrame({
                                        'ç±»åˆ«': [d['name'].split(' ')[0] for d in cat_data],
                                        'æ¦‚çŽ‡': [d['probability'] for d in cat_data]
                                    })
                                st.bar_chart(fig_data.set_index('ç±»åˆ«'))
                            
                            # æ˜¾ç¤ºå±žæ€§é¢„æµ‹ç»“æžœ
                            st.markdown("**é¢„æµ‹å±žæ€§åŠæ¦‚çŽ‡:**")
                            
                            # è®¡ç®—æ˜¾ç¤ºå¤šå°‘åˆ—
                            num_columns = 3  # é»˜è®¤3åˆ—æ˜¾ç¤º
                            
                            # æ ¹æ®é˜ˆå€¼ç­›é€‰å±žæ€§ï¼ˆé»˜è®¤0.5ï¼Œä½†è¿™é‡Œæ˜¾ç¤ºæ‰€æœ‰ï¼‰
                            # st.sliderå¯ä»¥è®©ç”¨æˆ·è°ƒæ•´ç­›é€‰é˜ˆå€¼
                            threshold = st.slider("å±žæ€§ç½®ä¿¡åº¦é˜ˆå€¼", min_value=0.0, max_value=1.0, value=0.3, step=0.05)
                            filtered_attrs = [attr for attr in attr_data if attr['probability'] >= threshold]
                            
                            if filtered_attrs:
                                # æŒ‰æ¦‚çŽ‡åˆ†ç»„æ˜¾ç¤ºå±žæ€§
                                # é«˜æ¦‚çŽ‡ç»„ (>0.7)
                                high_prob_attrs = [attr for attr in filtered_attrs if attr['probability'] > 0.7]
                                if high_prob_attrs:
                                    st.markdown("##### é«˜ç½®ä¿¡åº¦å±žæ€§ (>70%)")
                                    rows = (len(high_prob_attrs) + num_columns - 1) // num_columns 
                                    for r in range(rows):
                                        cols_attr = st.columns(num_columns)
                                        for c in range(num_columns):
                                            idx = r * num_columns + c
                                            if idx < len(high_prob_attrs):
                                                attr = high_prob_attrs[idx]
                                                with cols_attr[c]:
                                                    st.success(f"{attr['name']} ({attr['probability']*100:.1f}%)")
                                
                                # ä¸­æ¦‚çŽ‡ç»„ (0.5-0.7)
                                medium_prob_attrs = [attr for attr in filtered_attrs if 0.5 <= attr['probability'] <= 0.7]
                                if medium_prob_attrs:
                                    st.markdown("##### ä¸­ç­‰ç½®ä¿¡åº¦å±žæ€§ (50%-70%)")
                                    rows = (len(medium_prob_attrs) + num_columns - 1) // num_columns 
                                    for r in range(rows):
                                        cols_attr = st.columns(num_columns)
                                        for c in range(num_columns):
                                            idx = r * num_columns + c
                                            if idx < len(medium_prob_attrs):
                                                attr = medium_prob_attrs[idx]
                                                with cols_attr[c]:
                                                    st.info(f"{attr['name']} ({attr['probability']*100:.1f}%)")
                                
                                # ä½Žæ¦‚çŽ‡ç»„ (é˜ˆå€¼-0.5)
                                low_prob_attrs = [attr for attr in filtered_attrs if attr['probability'] < 0.5]
                                if low_prob_attrs:
                                    st.markdown("##### ä½Žç½®ä¿¡åº¦å±žæ€§ (<50%)")
                                    rows = (len(low_prob_attrs) + num_columns - 1) // num_columns 
                                    for r in range(rows):
                                        cols_attr = st.columns(num_columns)
                                        for c in range(num_columns):
                                            idx = r * num_columns + c
                                            if idx < len(low_prob_attrs):
                                                attr = low_prob_attrs[idx]
                                                with cols_attr[c]:
                                                    st.warning(f"{attr['name']} ({attr['probability']*100:.1f}%)")
                            else:
                                st.write("åœ¨å½“å‰é˜ˆå€¼ä¸‹æœªæ£€æµ‹åˆ°æ˜¾è‘—å±žæ€§ã€‚")
                            
                            # æ˜¾ç¤ºæ‰€æœ‰å±žæ€§çš„è¡¨æ ¼è§†å›¾ï¼ˆå¯æŠ˜å ï¼‰
                            with st.expander("æŸ¥çœ‹æ‰€æœ‰å±žæ€§æ¦‚çŽ‡è¯¦æƒ…"):
                                attr_df = pd.DataFrame(attr_data)
                                attr_df.columns = ["ç´¢å¼•", "å±žæ€§åç§°", "æ¦‚çŽ‡"]
                                attr_df["æ¦‚çŽ‡"] = attr_df["æ¦‚çŽ‡"].apply(lambda x: f"{x*100:.1f}%")
                                st.dataframe(attr_df)

                        st.success("è¯†åˆ«å®Œæˆï¼")

                    except Exception as e:
                        st.error(f"è¯†åˆ«è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                        
                        st.error(traceback.format_exc()) # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ 