import streamlit as st
import torch
import math
import os
import json
import traceback
import pandas as pd
from datetime import datetime
import re

from utils.state_manager import append_log

def generate_diagnostic_report(history_df, best_val_loss, total_epochs):
    """æ ¹æ®è®­ç»ƒå†å²ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
    report = []
    report.append("### ğŸ©º è®­ç»ƒè¯Šæ–­æŠ¥å‘Š")

    if history_df is None or history_df.empty:
        report.append("- âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼šç¼ºå°‘è®­ç»ƒå†å²æ•°æ®ã€‚")
        return "\n".join(report)

    final_epoch_data = history_df.iloc[-1]
    best_epoch_data = history_df.loc[history_df['Validation Loss'].idxmin()] if 'Validation Loss' in history_df.columns and history_df['Validation Loss'].notna().any() else None

    # 1. æ•´ä½“è¡¨ç°
    report.append(f"- **è®­ç»ƒè½®æ•°:** {len(history_df)} / {total_epochs}")
    if best_epoch_data is not None:
        report.append(f"- **æœ€ä½³éªŒè¯æŸå¤±:** {best_epoch_data['Validation Loss']:.4f} (å‡ºç°åœ¨ Epoch {int(best_epoch_data['epoch'])})")
    else:
        report.append("- **æœ€ä½³éªŒè¯æŸå¤±:** æœªè®°å½•æˆ–æ— æ•ˆã€‚")
    report.append(f"- **æœ€ç»ˆéªŒè¯æŸå¤±:** {final_epoch_data['Validation Loss']:.4f}")
    report.append(f"- **æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡:** {final_epoch_data['Validation Accuracy (%)']:.2f}%")

    # 2. æ”¶æ•›æ€§åˆ†æ
    if len(history_df) >= 5:
        last_5_val_loss = history_df['Validation Loss'].tail(5)
        if last_5_val_loss.is_monotonic_decreasing:
            report.append("- **æ”¶æ•›æ€§:** âœ… éªŒè¯æŸå¤±åœ¨æœ€å5è½®æŒç»­ä¸‹é™ï¼Œå¯èƒ½ä»æœ‰æå‡ç©ºé—´ã€‚")
        elif last_5_val_loss.iloc[-1] < last_5_val_loss.iloc[0]:
            report.append("- **æ”¶æ•›æ€§:** âš ï¸ éªŒè¯æŸå¤±åœ¨æœ€å5è½®æœ‰æ‰€æ³¢åŠ¨ï¼Œä½†æ•´ä½“ä»åœ¨ä¸‹é™ã€‚")
        else:
             report.append("- **æ”¶æ•›æ€§:** âŒ éªŒè¯æŸå¤±åœ¨æœ€å5è½®æœªèƒ½æŒç»­ä¸‹é™ï¼Œå¯èƒ½å·²æ”¶æ•›æˆ–é‡åˆ°ç“¶é¢ˆã€‚")
    else:
        report.append("- **æ”¶æ•›æ€§:** âš ï¸ è®­ç»ƒè½®æ•°è¾ƒå°‘ï¼Œéš¾ä»¥åˆ¤æ–­æ”¶æ•›è¶‹åŠ¿ã€‚")

    # 3. è¿‡æ‹Ÿåˆé£é™©
    train_loss_final = final_epoch_data.get('Train Loss', float('nan'))
    val_loss_final = final_epoch_data.get('Validation Loss', float('nan'))
    train_acc_final = final_epoch_data.get('Train Accuracy (%)', float('nan'))
    val_acc_final = final_epoch_data.get('Validation Accuracy (%)', float('nan'))

    loss_diff = abs(train_loss_final - val_loss_final) if math.isfinite(train_loss_final) and math.isfinite(val_loss_final) else float('inf')
    acc_diff = abs(train_acc_final - val_acc_final) if math.isfinite(train_acc_final) and math.isfinite(val_acc_final) else float('inf')

    # è®¾å®šä¸€äº›ç®€å•çš„é˜ˆå€¼
    overfitting_risk = "ä½"
    if loss_diff > 0.5 or acc_diff > 15:
        overfitting_risk = "é«˜"
    elif loss_diff > 0.2 or acc_diff > 8:
        overfitting_risk = "ä¸­"

    report.append(f"- **è¿‡æ‹Ÿåˆé£é™©:** {overfitting_risk} (åŸºäºæœ€ç»ˆæŸå¤±å·®å¼‚ {loss_diff:.2f} å’Œå‡†ç¡®ç‡å·®å¼‚ {acc_diff:.1f}%) ")
    if overfitting_risk != "ä½":
        report.append("  - _å»ºè®®: å¯å°è¯•å¢åŠ æ­£åˆ™åŒ–ã€æ•°æ®å¢å¼ºæˆ–æå‰åœæ­¢ã€‚_")

    return "\n".join(report)

def run_functional_test(model_save_dir, model_name, model_config, device):
    """å°è¯•åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¿›è¡Œä¸€æ¬¡æ¨¡æ‹Ÿæ¨ç†ï¼ŒåŒæ—¶æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶"""
    report = ["### âš™ï¸ åŠŸèƒ½æ¨¡æ‹Ÿæµ‹è¯•"]
    best_model_file = None
    metadata_file = None
    error_details = {}
    
    try:
        # æå–æ¨¡å‹åç§°çš„åŸºç¡€éƒ¨åˆ†ï¼Œå¿½ç•¥å¯èƒ½çš„æ—¶é—´æˆ³
        # ä¾‹å¦‚ï¼šä» "MD_RESNET18_5_64_50E04_0503_1137" æå– "MD_RESNET18_5_64_50E04"
        base_model_name = model_name
        # å¦‚æœæœ‰æ—¶é—´æˆ³æ ¼å¼ (_MMDD_HHMM)ï¼Œå»é™¤å®ƒ
        time_stamp_pattern = r'_\d{4}_\d{4}$'
        base_model_name = re.sub(time_stamp_pattern, '', base_model_name)
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶çš„æ–¹å¼æ›´çµæ´»
        all_files = os.listdir(model_save_dir)
        # 1. é¦–å…ˆå°è¯•å®Œå…¨åŒ¹é…
        possible_files = [f for f in all_files if f.startswith(f"best_model_{model_name}") and f.endswith(".pth")]
        
        # 2. å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä½¿ç”¨åŸºç¡€åç§°éƒ¨åˆ†åŒ¹é…
        if not possible_files:
            possible_files = [f for f in all_files if f.startswith(f"best_model_{base_model_name}") and f.endswith(".pth")]
            
        # 3. å¦‚æœè¿˜æ‰¾ä¸åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼ˆé€‚ç”¨äºåç§°è¢«æˆªæ–­çš„æƒ…å†µï¼‰
        if not possible_files and len(base_model_name) > 15:
            short_name = base_model_name[:15]  # å–å‰15ä¸ªå­—ç¬¦
            possible_files = [f for f in all_files if f.startswith(f"best_model_{short_name}") and f.endswith(".pth")]
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½æ‰¾ä¸åˆ°æ–‡ä»¶
        if not possible_files:
            report.append("- âŒ æœªæ‰¾åˆ°ä¿å­˜çš„æœ€ä½³æ¨¡å‹æ–‡ä»¶ã€‚")
            error_details["error_type"] = "missing_model_file"
            error_details["message"] = f"æ— æ³•åœ¨ {model_save_dir} ä¸­æ‰¾åˆ°ä¸ '{model_name}' ç›¸å…³çš„æ¨¡å‹æ–‡ä»¶"
            error_details["solution"] = "è¯·æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸå®Œæˆï¼Œæˆ–è€…æ¨¡å‹æ–‡ä»¶æ˜¯å¦è¢«æ„å¤–åˆ é™¤ã€‚å¯ä»¥å°è¯•é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
            return "\n".join(report), False, error_details
            
        # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…æ–‡ä»¶ï¼Œé€‰æ‹©æœ€æ–°çš„ä¸€ä¸ªï¼ˆæŒ‰epochæ’åºï¼‰
        try:
            # é¦–å…ˆå°è¯•æå–epochç¼–å·è¿›è¡Œæ’åº
            possible_files.sort(key=lambda x: int(x.split('_epoch')[-1].split('.')[0]), reverse=True)
        except (IndexError, ValueError):
            # å¦‚æœæ— æ³•æå–epochç¼–å·ï¼Œåˆ™æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åº
            possible_files.sort(key=lambda x: os.path.getmtime(os.path.join(model_save_dir, x)), reverse=True)
        
        best_model_file = os.path.join(model_save_dir, possible_files[0])
        report.append(f"- æ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶: `{best_model_file}`")
        
        # åŒæ ·ä½¿ç”¨æ›´çµæ´»çš„æ–¹å¼æŸ¥æ‰¾å…ƒæ•°æ®æ–‡ä»¶
        possible_metadata_files = [
            os.path.join(model_save_dir, f"{model_name}_metadata.json"),
            os.path.join(model_save_dir, f"{base_model_name}_metadata.json")
        ]
        
        metadata_file = None
        for meta_path in possible_metadata_files:
            if os.path.exists(meta_path):
                metadata_file = meta_path
                break

        # è¯»å–å¹¶æ˜¾ç¤ºåŸºæœ¬å…ƒæ•°æ®ä¿¡æ¯
        if metadata_file:
            report.append(f"- âœ… æ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶: `{metadata_file}`")
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                report.append("- å…ƒæ•°æ®æ¦‚è¦:")
                report.append(f"  - æ¨¡å‹åç§°: {metadata.get('model_name', 'æœªå®šä¹‰')}")
                report.append(f"  - ç‰ˆæœ¬: {metadata.get('version', 'æœªå®šä¹‰')}")
                report.append(f"  - æ¶æ„: {metadata.get('architecture', 'æœªå®šä¹‰')}")
                report.append(f"  - è¾“å…¥å½¢çŠ¶: {metadata.get('input_shape', 'æœªå®šä¹‰')}")
                report.append(f"  - æ”¯æŒç±»åˆ«æ•°: {len(metadata.get('class_names', []))} ç±»")
                report.append(f"  - æ”¯æŒç‰¹å¾æ•°: {len(metadata.get('feature_names', []))} é¡¹")
                report.append(f"  - åˆ›å»ºæ—¥æœŸ: {metadata.get('date_created', 'æœªå®šä¹‰')}")
            except Exception as e:
                report.append(f"- âš ï¸ è¯»å–å…ƒæ•°æ®æ–‡ä»¶å‡ºé”™: {e}")
                report.append(f"  - å¯èƒ½åŸå› : å…ƒæ•°æ®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æˆ–æŸå")
                report.append(f"  - å»ºè®®è§£å†³æ–¹æ¡ˆ: é‡æ–°ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶")
                error_details["error_type"] = "metadata_read_error"
                error_details["message"] = f"è¯»å–å…ƒæ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}"
                error_details["solution"] = "å…ƒæ•°æ®æ–‡ä»¶å¯èƒ½å·²æŸåï¼Œè¯·å°è¯•ä½¿ç”¨'æ‰¹é‡ç”Ÿæˆç¼ºå¤±çš„å…ƒæ•°æ®'åŠŸèƒ½é‡æ–°ç”Ÿæˆã€‚"
        else:
            report.append("- âš ï¸ æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„å¯ç”¨æ€§ã€‚")
            report.append("  - è§£å†³æ–¹æ¡ˆ: ç‚¹å‡»'æ‰¹é‡ç”Ÿæˆç¼ºå¤±çš„å…ƒæ•°æ®'æŒ‰é’®æˆ–ä½¿ç”¨æ¨¡å‹ç®¡ç†ç•Œé¢ç”Ÿæˆå…ƒæ•°æ®")
            error_details["missing_metadata"] = True

        # åŠ è½½æ¨¡å‹
        report.append("- å°è¯•åŠ è½½æ¨¡å‹...")
        try:
            from model import ClothesModel
            model = ClothesModel(num_categories=model_config['num_categories'], backbone=model_config['backbone'])
        except Exception as model_init_error:
            report.append(f"- âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_init_error}")
            report.append("  - å¯èƒ½åŸå› : æ¨¡å‹ç±»å®šä¹‰å·²æ›´æ”¹æˆ–å‚æ•°ä¸åŒ¹é…")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: è¯·æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œç¡®ä¿backboneå‚æ•°æ­£ç¡®")
            error_details["error_type"] = "model_init_error"
            error_details["message"] = f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_init_error}"
            error_details["solution"] = "è¯·æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œå°¤å…¶æ˜¯backboneå‚æ•°æ˜¯å¦æ­£ç¡®ã€‚å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
            return "\n".join(report), False, error_details
            
        try:
            model.load_state_dict(torch.load(best_model_file, map_location=device))
        except Exception as load_error:
            report.append(f"- âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {load_error}")
            report.append("  - å¯èƒ½åŸå› : æ¨¡å‹æ–‡ä»¶æŸåæˆ–æ¨¡å‹ç»“æ„ä¸ä¿å­˜æ—¶ä¸ä¸€è‡´")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼Œå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹")
            error_details["error_type"] = "model_load_error"
            error_details["message"] = f"æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {load_error}"
            error_details["solution"] = "æ¨¡å‹æ–‡ä»¶å¯èƒ½å·²æŸåæˆ–ä¸å½“å‰ä»£ç ä¸å…¼å®¹ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ï¼Œå¿…è¦æ—¶é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
            return "\n".join(report), False, error_details
            
        model.to(device)
        model.eval()
        report.append("- âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚")

        # æ¨¡æ‹Ÿæ¨ç†
        report.append("- å°è¯•æ¨¡æ‹Ÿæ¨ç†...")
        # åˆ›å»ºè™šæ‹Ÿè¾“å…¥ (batch_size=1, channels=3, height=224, width=224)
        try:
            dummy_input = torch.randn(1, 3, 224, 224).to(device)
            with torch.no_grad():
                cat_logits, attr_logits = model(dummy_input)
        except RuntimeError as rt_error:
            # CUDAå†…å­˜ä¸è¶³
            if "CUDA out of memory" in str(rt_error):
                report.append(f"- âŒ æ¨ç†å¤±è´¥: GPUå†…å­˜ä¸è¶³")
                report.append("  - åŸå› : å½“å‰GPUå†…å­˜ä¸è¶³ä»¥è¿è¡Œæ­¤æ¨¡å‹")
                report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: å…³é—­å…¶ä»–å ç”¨GPUå†…å­˜çš„åº”ç”¨ï¼Œæˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
                error_details["error_type"] = "cuda_oom_error"
                error_details["message"] = f"GPUå†…å­˜ä¸è¶³: {rt_error}"
                error_details["solution"] = "è¯·å°è¯•å…³é—­å…¶ä»–å ç”¨GPUå†…å­˜çš„åº”ç”¨ç¨‹åºï¼Œæˆ–è€…ä½¿ç”¨å†…å­˜å ç”¨æ›´å°çš„backboneã€‚"
            else:
                report.append(f"- âŒ æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°è¿è¡Œæ—¶é”™è¯¯: {rt_error}")
                report.append("  - å¯èƒ½åŸå› : æ¨¡å‹ç»“æ„ä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´æˆ–è¾“å…¥æ ¼å¼é—®é¢˜")
                report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œç¡®ä¿backboneå‚æ•°æ­£ç¡®")
                error_details["error_type"] = "inference_runtime_error"
                error_details["message"] = f"æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°è¿è¡Œæ—¶é”™è¯¯: {rt_error}"
                error_details["solution"] = "è¯·æ£€æŸ¥æ¨¡å‹é…ç½®ä¸è®­ç»ƒæ—¶æ˜¯å¦ä¸€è‡´ï¼Œå°¤å…¶æ˜¯backboneå‚æ•°ã€‚å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
            return "\n".join(report), False, error_details
        except Exception as infer_error:
            report.append(f"- âŒ æ¨ç†å¤±è´¥: {infer_error}")
            report.append("  - å¯èƒ½åŸå› : æ¨¡å‹ç»“æ„é—®é¢˜æˆ–å†…éƒ¨é”™è¯¯")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥æ¨¡å‹ä»£ç ï¼Œå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹")
            error_details["error_type"] = "inference_error"
            error_details["message"] = f"æ¨ç†å¤±è´¥: {infer_error}"
            error_details["solution"] = "æ¨¡å‹å†…éƒ¨å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ä»£ç æˆ–å°è¯•é‡æ–°è®­ç»ƒã€‚"
            return "\n".join(report), False, error_details
        
        # æ£€æŸ¥è¾“å‡º
        report.append(f"- æ¨¡å‹è¾“å‡º (ç±»åˆ« Logits): {cat_logits.shape}")
        report.append(f"- æ¨¡å‹è¾“å‡º (å±æ€§ Logits): {attr_logits.shape}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_categories = model_config['num_categories']
        expected_attributes = 26  # å›ºå®š26ä¸ªå±æ€§
        
        shape_errors = []
        if cat_logits.shape[0] != 1:
            shape_errors.append(f"ç±»åˆ«è¾“å‡ºæ‰¹æ¬¡ç»´åº¦é”™è¯¯: æœŸæœ›1ï¼Œå®é™…{cat_logits.shape[0]}")
        if cat_logits.shape[1] != expected_categories:
            shape_errors.append(f"ç±»åˆ«æ•°é‡é”™è¯¯: æœŸæœ›{expected_categories}ï¼Œå®é™…{cat_logits.shape[1]}")
        if attr_logits.shape[0] != 1:
            shape_errors.append(f"å±æ€§è¾“å‡ºæ‰¹æ¬¡ç»´åº¦é”™è¯¯: æœŸæœ›1ï¼Œå®é™…{attr_logits.shape[0]}")
        if attr_logits.shape[1] != expected_attributes:
            shape_errors.append(f"å±æ€§æ•°é‡é”™è¯¯: æœŸæœ›{expected_attributes}ï¼Œå®é™…{attr_logits.shape[1]}")
            
        if shape_errors:
            report.append("- âŒ æ¨¡æ‹Ÿæ¨ç†å®Œæˆï¼Œä½†è¾“å‡ºå½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸï¼")
            for err in shape_errors:
                report.append(f"  - {err}")
            report.append("  - å¯èƒ½åŸå› : æ¨¡å‹è®­ç»ƒæ—¶çš„ç±»åˆ«æˆ–å±æ€§æ•°é‡é…ç½®ä¸å½“å‰ä¸ä¸€è‡´")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥è®­ç»ƒé…ç½®ï¼Œç¡®ä¿ç±»åˆ«æ•°é‡å’Œå±æ€§æ•°é‡æ­£ç¡®è®¾ç½®")
            error_details["error_type"] = "output_shape_error"
            error_details["message"] = "æ¨¡å‹è¾“å‡ºå½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸ"
            error_details["details"] = shape_errors
            error_details["solution"] = "æ¨¡å‹è¾“å‡ºå°ºå¯¸ä¸é¢„æœŸä¸ç¬¦ï¼Œå¯èƒ½æ˜¯è®­ç»ƒé…ç½®ä¸å½“å‰ä¸åŒ¹é…ã€‚è¯·æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œå°¤å…¶æ˜¯ç±»åˆ«æ•°é‡è®¾ç½®ã€‚"
            return "\n".join(report), False, error_details
        else:
            report.append("- âœ… æ¨¡æ‹Ÿæ¨ç†æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶ç¬¦åˆé¢„æœŸã€‚")
             
        # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®å’Œæ¨¡å‹ï¼Œå…¨éƒ¨æˆåŠŸæ‰è¿”å›æˆåŠŸ
        if os.path.exists(metadata_file):
            report.append("- ğŸ‰ åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼æ¨¡å‹å¯ä»¥æ­£å¸¸åŠ è½½å’Œæ¨ç†ï¼Œå…ƒæ•°æ®æ–‡ä»¶é½å…¨ã€‚")
            return "\n".join(report), True, {}
        else:
            report.append("- âš ï¸ åŠŸèƒ½æµ‹è¯•éƒ¨åˆ†æˆåŠŸï¼Œä½†ç¼ºå°‘å…ƒæ•°æ®æ–‡ä»¶ã€‚")
            report.append("  - å»ºè®®: ç‚¹å‡»'æ‰¹é‡ç”Ÿæˆç¼ºå¤±çš„å…ƒæ•°æ®'æŒ‰é’®ç”Ÿæˆç¼ºå¤±çš„å…ƒæ•°æ®æ–‡ä»¶")
            error_details["error_type"] = "missing_metadata"
            error_details["message"] = "æ¨¡å‹å¯ä»¥æ­£å¸¸åŠ è½½å’Œä½¿ç”¨ï¼Œä½†ç¼ºå°‘å…ƒæ•°æ®æ–‡ä»¶"
            error_details["solution"] = "è¯·ä½¿ç”¨'æ‰¹é‡ç”Ÿæˆç¼ºå¤±çš„å…ƒæ•°æ®'åŠŸèƒ½ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶ã€‚"
            return "\n".join(report), False, error_details

    except Exception as e:
        error_trace = traceback.format_exc()
        report.append(f"- âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        report.append(f"- é”™è¯¯ç±»å‹: {type(e).__name__}")
        report.append("- å¯èƒ½åŸå› :")
        
        if "No such file or directory" in str(e):
            report.append("  - æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨ï¼Œæ¨¡å‹æ–‡ä»¶æˆ–ç›®å½•å¯èƒ½å·²è¢«åˆ é™¤")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹")
            error_details["error_type"] = "file_not_found"
        elif "CUDA out of memory" in str(e):
            report.append("  - GPUå†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½æˆ–è¿è¡Œæ¨¡å‹")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: å…³é—­å…¶ä»–å ç”¨GPUå†…å­˜çš„åº”ç”¨ï¼Œæˆ–ä½¿ç”¨CPUæ¨¡å¼")
            error_details["error_type"] = "cuda_oom"
        elif "ModuleNotFoundError" in error_trace:
            report.append("  - ç¼ºå°‘å¿…è¦çš„Pythonæ¨¡å—")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…")
            error_details["error_type"] = "missing_module"
        elif "KeyError" in error_trace:
            report.append("  - æ¨¡å‹å‚æ•°é”®é”™è¯¯ï¼Œæ¨¡å‹ç»“æ„å¯èƒ½ä¸ä¿å­˜æ—¶ä¸ä¸€è‡´")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: ç¡®ä¿ä½¿ç”¨ç›¸åŒç‰ˆæœ¬çš„ä»£ç åŠ è½½æ¨¡å‹ï¼Œæˆ–é‡æ–°è®­ç»ƒæ¨¡å‹")
            error_details["error_type"] = "model_key_error"
        else:
            report.append("  - æœªçŸ¥é”™è¯¯ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            report.append("  - å»ºè®®è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥ç¯å¢ƒé…ç½®ï¼Œæ¨¡å‹æ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹")
            error_details["error_type"] = "unknown_error"
            
        error_details["message"] = str(e)
        error_details["solution"] = "è¯·æ ¹æ®é”™è¯¯è¯¦æƒ…é‡‡å–å¯¹åº”è§£å†³æ–¹æ¡ˆï¼Œå¿…è¦æ—¶å¯å°è¯•é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
        append_log(f"åŠŸèƒ½æµ‹è¯•å¤±è´¥: {error_trace}")
        return "\n".join(report), False, error_details

def generate_metadata_for_model(model_result):
    """ä¸ºå•ä¸ªæ¨¡å‹ç”Ÿæˆå…ƒæ•°æ®"""
    try:
        model_name = model_result.get("model_name", "")
        model_path = model_result.get("best_model_path", "")
        if not model_path or not os.path.exists(model_path):
            return False, f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"

        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            "model_name": model_name,
            "version": "1.0.0",
            "description": f"åŸºäº{model_result.get('backbone', 'unknown')}çš„æœè£…åˆ†ç±»æ¨¡å‹",
            "architecture": model_result.get('backbone', 'unknown'),
            "input_shape": [3, 224, 224],  # æ ‡å‡†è¾“å…¥å°ºå¯¸
            "framework": "PyTorch",
            "date_created": model_result.get("start_time_str", "").split()[0],  # åªå–æ—¥æœŸéƒ¨åˆ†
            "trained_by": "å–µæ­æœè£…è¯†åˆ«è®­ç»ƒå¹³å°",
            "training_params": {
                "epochs": model_result.get("total_epochs"),
                "completed_epochs": model_result.get("completed_epochs"),
                "best_val_loss": model_result.get("best_val_loss"),
                "best_epoch": model_result.get("best_epoch"),
                "strategy": model_result.get("strategy"),
                "status": model_result.get("status"),
            },
            # æ·»åŠ é»˜è®¤çš„ç±»åˆ«å’Œç‰¹å¾åç§°
            "class_names": [
                "Tæ¤", "è¡¬è¡«", "å«è¡£", "æ¯›è¡£", "è¥¿è£…", "å¤¹å…‹", "ç¾½ç»’æœ", "é£è¡£",
                "ç‰›ä»”è£¤", "ä¼‘é—²è£¤", "è¥¿è£¤", "çŸ­è£¤", "è¿åŠ¨è£¤", "è¿è¡£è£™", "åŠèº«è£™",
                "æ——è¢", "ç¤¼æœ", "è¿åŠ¨é‹", "çš®é‹", "é«˜è·Ÿé‹", "é´å­", "å‡‰é‹", "æ‹–é‹",
                "å¸½å­", "å›´å·¾", "é¢†å¸¦", "æ‰‹å¥—", "è¢œå­", "è…°å¸¦", "çœ¼é•œ", "æ‰‹è¡¨",
                "é¡¹é“¾", "æ‰‹é“¾", "è€³ç¯", "æˆ’æŒ‡", "åŒ…åŒ…", "èƒŒåŒ…", "æ‰‹æåŒ…", "é’±åŒ…", "è¡Œæç®±"
            ],
            "feature_names": [
                "é¢œè‰²", "æè´¨", "æ ·å¼", "èŠ±çº¹", "å­£èŠ‚", "æ­£å¼åº¦", "é¢†å‹", "è¢–é•¿",
                "é•¿åº¦", "è£¤å‹", "é‹å‹", "é«˜åº¦", "é—­åˆæ–¹å¼"
            ]
        }

        # ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
        model_dir = os.path.dirname(model_path)
        metadata_file = os.path.join(model_dir, f"{model_name}_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        
        return True, metadata_file
    except Exception as e:
        return False, f"ç”Ÿæˆå…ƒæ•°æ®æ—¶å‡ºé”™: {e}"

def batch_generate_metadata():
    """æ‰¹é‡ä¸ºæ‰€æœ‰ç¼ºå¤±å…ƒæ•°æ®çš„æ¨¡å‹ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶"""
    from utils.file_utils import load_results
    
    results = []
    all_models = load_results()
    
    for model_result in all_models:
        model_name = model_result.get("model_name", "æœªå‘½åæ¨¡å‹")
        model_path = model_result.get("best_model_path", "")
        
        if not model_path or not os.path.exists(model_path):
            results.append((model_name, False, "æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨"))
            continue
            
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å…ƒæ•°æ®
        model_dir = os.path.dirname(model_path)
        metadata_file = os.path.join(model_dir, f"{model_name}_metadata.json")
        
        # å¦‚æœæœ‰æ—¶é—´æˆ³å‘½åçš„æ¨¡å‹ï¼Œä¹Ÿå°è¯•æ£€æŸ¥ä¸å¸¦æ—¶é—´æˆ³çš„å…ƒæ•°æ®æ–‡ä»¶
        if not os.path.exists(metadata_file):
            # å°è¯•ç§»é™¤æ—¶é—´æˆ³éƒ¨åˆ†å¹¶æ£€æŸ¥
            base_model_name = re.sub(r'_\d{4}_\d{4}$', '', model_name)
            alt_metadata_file = os.path.join(model_dir, f"{base_model_name}_metadata.json")
            if os.path.exists(alt_metadata_file):
                results.append((model_name, True, f"æ‰¾åˆ°ä½¿ç”¨åŸºç¡€åç§°çš„å…ƒæ•°æ®: {alt_metadata_file}"))
                continue
        else:
            results.append((model_name, True, "å…ƒæ•°æ®å·²å­˜åœ¨"))
            continue
            
        # ç”Ÿæˆå…ƒæ•°æ®
        success, message = generate_metadata_for_model(model_result)
        results.append((model_name, success, message))
    
    return results

def create_metadata_file(model_name, model_data, metadata_input):
    """æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶"""
    try:
        model_path = model_data.get("best_model_path", "")
        if not model_path or not os.path.exists(model_path):
            return False, f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"
            
        # è§£æè¾“å…¥å‚æ•°
        version = metadata_input.get("version", "1.0.0")
        description = metadata_input.get("description", f"åŸºäº{model_data.get('backbone', 'unknown')}çš„æœè£…åˆ†ç±»æ¨¡å‹")
        trained_by = metadata_input.get("trained_by", "å–µæ­æœè£…è¯†åˆ«è®­ç»ƒå¹³å°")
        date_created = metadata_input.get("date_created", datetime.now().strftime("%Y-%m-%d"))
        
        # è§£æè¾“å…¥å½¢çŠ¶
        input_shape_str = metadata_input.get("input_shape", "3,224,224")
        input_shape = [int(x.strip()) for x in input_shape_str.split(",")]
        
        # è§£æç±»åˆ«å’Œç‰¹å¾åç§°
        class_names_text = metadata_input.get("class_names", "")
        class_names = [name.strip() for name in class_names_text.split("\n") if name.strip()]
        
        feature_names_text = metadata_input.get("feature_names", "")
        feature_names = [name.strip() for name in feature_names_text.split("\n") if name.strip()]
        
        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            "model_name": model_name,
            "version": version,
            "description": description,
            "architecture": model_data.get('backbone', 'unknown'),
            "input_shape": input_shape,
            "framework": "PyTorch",
            "date_created": date_created,
            "trained_by": trained_by,
            "training_params": {
                "epochs": model_data.get("total_epochs"),
                "completed_epochs": model_data.get("completed_epochs"),
                "best_val_loss": model_data.get("best_val_loss"),
                "best_epoch": model_data.get("best_epoch"),
                "strategy": model_data.get("strategy"),
                "status": model_data.get("status"),
            },
            "class_names": class_names,
            "feature_names": feature_names
        }
        
        # ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
        model_dir = os.path.dirname(model_path)
        metadata_file = os.path.join(model_dir, f"{model_name}_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
            
        return True, f"å·²æˆåŠŸåˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶: {metadata_file}"
    except Exception as e:
        return False, f"åˆ›å»ºå…ƒæ•°æ®æ—¶å‡ºé”™: {e}" 